import pandas as pd
import os

# Base path to the folder containing the year folders
base_path = '../TFG/data/'  # Update with the actual path

# Initialize an empty list to store DataFrames
all_data = []

# Walk through each year folder and process files
for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.endswith('.1'): # or file.endswith('.2'):  # Filter for files with the .1 or .2 extension
            # Extract year and file date from the filename
            file_path = os.path.join(root, file)
            base_name = os.path.basename(file)
            file_date = base_name.split('_')[1].split('.')[0]
            
            # Step 4: Read the file, skip the first line and remove the last line
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Remove the first line (MARGINALPDBC;) and the last line (*)
            data_lines = lines[1:-1]
            
            # Convert the list of strings into a DataFrame
            daily_data = pd.DataFrame([line.strip().split(';') for line in data_lines])
            
            # Check if there are extra columns (caused by trailing ';')
            if daily_data.shape[1] > 6:
                # Drop any extra columns that might have been added
                daily_data = daily_data.iloc[:, :6]
            
            # Assign column names
            daily_data.columns = ['Year', 'Month', 'Day', 'Hour', 'MarginalPT', 'MarginalES']
            
            # Remove rows with invalid data
            daily_data = daily_data[daily_data['Year'].str.isdigit()]  # Only keep rows where 'Year' is a number
            
            # Convert the columns to the appropriate data types
            daily_data = daily_data.astype({
                'Year': int, 'Month': int, 'Day': int, 'Hour': int, 
                'MarginalPT': float, 'MarginalES': float
            })
            
            # Append daily data to the list
            all_data.append(daily_data)

# Step 8: Concatenate all daily DataFrames into one big DataFrame
full_data = pd.concat(all_data, ignore_index=True)

# Step 9: Create a datetime column from Year, Month, Day, and Hour
# Adjust the 'Hour' column to deal with the potential 25th hour issue
full_data['Hour'] = full_data['Hour'].apply(lambda x: 0 if x == 25 else x)
full_data['Datetime'] = pd.to_datetime(full_data[['Year', 'Month', 'Day', 'Hour']], errors='coerce')

# Step 10: Set the 'Datetime' column as the index
full_data.set_index('Datetime', inplace=True)

# Optional: Drop unnecessary columns
full_data.drop(['Year', 'Month', 'Day', 'Hour'], axis=1, inplace=True)

# Step 11: Check the resulting DataFrame
# print(full_data.head())

# print(full_data.isnull().sum()) # Check for missing values

full_data.to_csv('data/raw_data.csv')

# Load the dataset (assuming it's in a CSV file)
df = pd.read_csv('data/raw_data.csv')

# Convert the 'Datetime' column to datetime format if it isn't already
df['Datetime'] = pd.to_datetime(df['Datetime'])

# Sort the data by the 'Datetime' column
df = df.sort_values(by='Datetime')

# Drop MarginalPT
df.drop('MarginalPT', axis=1, inplace=True)

# Save the sorted data to a new file if needed
df.to_csv('data/processed_data.csv', index=False)

# Read the CSV file
df = pd.read_csv('data/processed_data.csv')

# Convert 'Datetime' column to datetime objects
df['Datetime'] = pd.to_datetime(df['Datetime'])

# Generate a complete range of hourly timestamps
full_range = pd.date_range(start=df['Datetime'].min(), end=df['Datetime'].max(), freq='h')

# Find missing timestamps
missing_timestamps = full_range.difference(df['Datetime'])

# If successful, print a message
if missing_timestamps.empty:
    print("Data processing completed successfully.")

# Else, print an error message
else:
    print("Error: Missing timestamps found. Please check the data.")

# Print missing timestamps
print("Missing timestamps:")
print(missing_timestamps)