{"cells":[{"cell_type":"markdown","metadata":{"id":"B8UQ3_oyam7E"},"source":["## **Objective**\n","This notebook is the first part of two lab assignments in which we will learn how to solve:\n","- **Binary classification problems** in practice (Part I).\n","- **Multiclass classification problems** in practice (Part II).\n","\n","\n","As we learnt in the master classes, the goal of a classification problem is to assign a class or category to every instance or sample of a data collection.\n","\n","Here, we will assume that:\n","\n","*   Every sample ${\\bf x}$ is an $N$-dimensional vector in $\\mathcal{R}^N$, where $N$ is the number of features.\n","*   The class $y$ of sample ${\\bf x}$ is an element of a binary set $\\{0,1\\}$.\n","\n","Then, the goal of a classifier is to predict the true value of $y$ after observing ${\\bf x}$.\n","\n","We will denote as $\\hat{y}$ the classifier output or decision. If $y=\\hat{y}$, the decision is a **hit**; otherwise it is an **error**.\n","\n","First of all, it is necessary to decide which dataset will be used in the experiments. For this purpose, **Heart Attack Analysis & Prediction Dataset** is the dataset that will be employed throughout this first lab assignment. This dataset provides us with a series of risk factors for different patients (e.g., age, sex, whether the patient has chest pain, etc.) for who the chances of suffering a heart attack have been annotated. Hence, our **goal** here is to **develop a classifier** model that learns to **predict the chances of individuals** with specific ages, sex, etc., **suffering a heart attack** (binary classification)."]},{"cell_type":"markdown","metadata":{"id":"KsREsuk-O4r8"},"source":["# **Modern Theory of Detection and Estimation**\n","## **Lab 2. Machine Learning for Classification: Binary Classification (Part I)**\n","### **Academic Year 2022/2023** \n","\n","Bachelor's Degree in:\n","*   Mobile and Space Communications Engineering (group 61)\n","*   Sound and Image Engineering (group 66)\n","*   Telecomunication Technologies Engineering (groups 91 and 92)\n","*   Telematic Engineering (group 71)\n","\n","Signal Theory and Communications Department"]},{"cell_type":"markdown","metadata":{"id":"EZz4PZAlzSLP"},"source":["Let's import some Python standard libraries!"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Qz8T8x3s08TY","executionInfo":{"status":"ok","timestamp":1669817436274,"user_tz":-60,"elapsed":1846,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}}},"outputs":[],"source":["# Common imports \n","import os\n","import numpy as np\n","import pandas as pd\n","from termcolor import colored\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Figures plotted inside the notebook\n","%matplotlib inline \n","# High quality figures\n","%config InlineBackend.figure_format = 'retina' \n","# For fancy table Display\n","%load_ext google.colab.data_table\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"MkRL7IHnmRJd"},"source":["# 1. Download the dataset"]},{"cell_type":"markdown","metadata":{"id":"LZmkV6SvPBkI"},"source":["## 1.1 Description of the dataset\n","\n","As previously mentioned, we will take the **Heart Attack Analysis & Prediction Dataset**, which contains information about different patients for who the chances of suffering a heart attack have been annoted. \n","\n","This dataset dates from $1988$ and consists of four datatases: Cleveland, Hungary, Switzerland and Long Beach V. The original dataset contains $76$ attributes or features, including the predicted attribute (that is, the class or category $-$or target), but all experiments refer to using a subset of $14$ of them (including the target), which are depicted below:\n","\n","*   age: age of the patient\n","*   sex: sex of the patient\n","      *   $0$: Female\n","      *   $1$: Male\n","*   cp: chest pain type\n","      *   $1$: Typical angina\n","      *   $2$: Atypical angina\n","      *   $3$: Non-anginal angina\n","      *   $4$: Asymptomatic\n","*   trtbps: resting blood pressure (in mm Hg)\n","*   chol: cholestoral in mg/dl fetched via BMI sensor\n","*   fbs: fasting blood sugar \n","      *   $0$: $>$ 120 mg/dl\n","      *   $1$: $<$ 120 mg/dl\n","*   restecg: resting electrocardiographic result\n","      *   $0$: Normal\n","      *   $1$: ST-T wave abnormality (T wave inversions and/or ST elevation or depression of $> 0.05$ mV)\n","      *   $2$: showing probable or definite left ventricular hypertrophy by Estes' criteria\n","*   thalachh: maximum heart rate achieved\n","*   exng: Exercise induced angina \n","      *   $0$: No\n","      *   $1$: Yes\n","*   oldpeak: ST depression induced by exercise relative to rest\n","*   slp: slope of the peak exercise ST segment\n","      *   $0$: Upsloping\n","      *   $1$: Flat\n","      *   $2$: Downsloping\n","*   caa: number of major vessels ($0-3$) colored by flourosopy\n","*   thall: thalium stress test\n","      *   $0$: normal\n","      *   $1$: fixed defect\n","      *   $2$: reversable defect\n","\n","The \"target\" field refers to the chance of suffering a heart attack. It is an integer value so that:\n","*   $0=$ less chance of heart attack\n","*   $1=$ more chance of heart attack\n","\n","$\\underline{\\text{Note}}$: The names and social security numbers of the patients were removed from the database, replaced with dummy values.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_GTEDEi1oQPT"},"source":["## 1.2 Getting the dataset\n","The first step in this assignment is to download the dataset we are going to work with. We will be using the **Heart Attack Analysis & Prediction Dataset**, which consists of a dataset for heart attack classification. This dataset is available from Kaggle, but we have already downloaded it for you in [this Drive folder](https://drive.google.com/drive/folders/1N-Gll-WhmnV4yWADcCKKNn6L6mkJu3ko?usp=share_link).\n","\n","Use the link above to download the folder: \n","\n","``heart-attack-analysis-prediction-dataset`` \n","\n","and save the file in a folder in your Google Drive. Then, in the following cell, fill in the variable ``path_to_folder`` with the name of the folder in your Google Drive in which you have saved the files. Please, note that the path to your Google Drive is given by:\n","\n","``\n","/content/drive/My Drive/\n","``\n","\n","Hence, if you want to save the results of this lab assignment into a folder in your Google Drive, called, for example,  ``TMDE``, then the variable ``path_to_folder`` should be:\n","\n","``\n","path_to_folder = '/content/drive/My Drive/TMDE' \n","``"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xmMRDD8LV1LD","executionInfo":{"status":"ok","timestamp":1669817436275,"user_tz":-60,"elapsed":7,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}}},"outputs":[],"source":["path_to_folder = '/content/drive/My Drive/TMDE'  # UPDATE THIS ACCORDING TO WHERE YOU HAVE SAVED THE DATABASE!"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"q39PSRmoQgG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817460074,"user_tz":-60,"elapsed":23805,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"70861b9c-b787-4f72-d305-88be995f8a0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","import os\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","os.chdir(path_to_folder) "]},{"cell_type":"markdown","metadata":{"id":"diTbXepH1LUT"},"source":["Once we have the dataset in our Google Drive, we can load it as a dataframe and inspect its first values as follows:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4LsdLVjv1SdE","colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"status":"ok","timestamp":1669817461147,"user_tz":-60,"elapsed":1076,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"e5685178-d7d7-4274-fd9c-958c33a4bfdf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n","0   63    1   3     145   233    1        0       150     0      2.3    0   \n","1   37    1   2     130   250    0        1       187     0      3.5    0   \n","2   41    0   1     130   204    0        0       172     0      1.4    2   \n","3   56    1   1     120   236    0        1       178     0      0.8    2   \n","4   57    0   0     120   354    0        1       163     1      0.6    2   \n","\n","   caa  thall  output  \n","0    0      1       1  \n","1    0      2       1  \n","2    0      2       1  \n","3    0      2       1  \n","4    0      2       1  "],"text/html":["\n","  <div id=\"df-7cceb96c-d0be-4acd-a249-6b2851c34fbf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trtbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalachh</th>\n","      <th>exng</th>\n","      <th>oldpeak</th>\n","      <th>slp</th>\n","      <th>caa</th>\n","      <th>thall</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>145</td>\n","      <td>233</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>150</td>\n","      <td>0</td>\n","      <td>2.3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>37</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>130</td>\n","      <td>250</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>187</td>\n","      <td>0</td>\n","      <td>3.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>130</td>\n","      <td>204</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>172</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>56</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>120</td>\n","      <td>236</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>178</td>\n","      <td>0</td>\n","      <td>0.8</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>120</td>\n","      <td>354</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>163</td>\n","      <td>1</td>\n","      <td>0.6</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cceb96c-d0be-4acd-a249-6b2851c34fbf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7cceb96c-d0be-4acd-a249-6b2851c34fbf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7cceb96c-d0be-4acd-a249-6b2851c34fbf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a8bd4d5e58f96183/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 63,\n            'f': \"63\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 145,\n            'f': \"145\",\n        },\n{\n            'v': 233,\n            'f': \"233\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 150,\n            'f': \"150\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2.3,\n            'f': \"2.3\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 37,\n            'f': \"37\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 130,\n            'f': \"130\",\n        },\n{\n            'v': 250,\n            'f': \"250\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 187,\n            'f': \"187\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 3.5,\n            'f': \"3.5\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 41,\n            'f': \"41\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 130,\n            'f': \"130\",\n        },\n{\n            'v': 204,\n            'f': \"204\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 172,\n            'f': \"172\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1.4,\n            'f': \"1.4\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 56,\n            'f': \"56\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 120,\n            'f': \"120\",\n        },\n{\n            'v': 236,\n            'f': \"236\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 178,\n            'f': \"178\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0.8,\n            'f': \"0.8\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 57,\n            'f': \"57\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 120,\n            'f': \"120\",\n        },\n{\n            'v': 354,\n            'f': \"354\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 163,\n            'f': \"163\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.6,\n            'f': \"0.6\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"age\"], [\"number\", \"sex\"], [\"number\", \"cp\"], [\"number\", \"trtbps\"], [\"number\", \"chol\"], [\"number\", \"fbs\"], [\"number\", \"restecg\"], [\"number\", \"thalachh\"], [\"number\", \"exng\"], [\"number\", \"oldpeak\"], [\"number\", \"slp\"], [\"number\", \"caa\"], [\"number\", \"thall\"], [\"number\", \"output\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "},"metadata":{},"execution_count":4}],"source":["data_df = pd.read_csv('heart-attack-analysis-prediction-dataset/heart.csv')\n","data_df.head()"]},{"cell_type":"markdown","metadata":{"id":"5G3Rs4TMA9yu"},"source":["## 1.3 Dataset analysis\n","\n","Let's check how many samples and features/attributes the dataset is composed of. We'll also inspect the  number of samples per category in the target variable (``output``)."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gYdpEqzp4J84","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817461147,"user_tz":-60,"elapsed":11,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"26db7eeb-820e-49ae-920b-b12e85b0a302"},"outputs":[{"output_type":"stream","name":"stdout","text":["(303, 14)\n","Index(['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n","       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'],\n","      dtype='object')\n","303\n"]}],"source":["# How many samples are there in the dataset? Print it out!\n","# YOUR CODE HERE\n","print(data_df.shape) # rows (samples), and columns (features)\n","print(data_df.keys())\n","print(data_df.shape[0])\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Qyy6bOy84ap3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817461148,"user_tz":-60,"elapsed":10,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"7a6c4052-0621-4c66-9867-091de159efd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["14\n"]}],"source":["# What is the number of features per sample? Print it out!\n","# YOUR CODE HERE\n","print(data_df.shape[-1])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"D14krOmW65KB","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1669817461682,"user_tz":-60,"elapsed":542,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"34f4d3af-fcda-4f0f-f7fe-514f5f2a12ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f77d9c990d0>"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"image/png":{"width":388,"height":261},"needs_background":"light"}}],"source":["# Plot an histogram with the number of samples per category in the target variable\n","# HINT: You can use the function sns.countplot\n","\n","# YOUR CODE HERE\n","sns.countplot(x = data_df['output']) # we printed out the labels\n"]},{"cell_type":"markdown","metadata":{"id":"ecU4koQCPa7A"},"source":["For now on, we are working with NumPy arrays, so let's save our data into:\n","*   $\\mathbf{X}$ that represents the matrix of features.\n","*   $\\mathbf{y}$ that represents a 1D array containing the target variable.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ri8oncmnPz0v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817461683,"user_tz":-60,"elapsed":8,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"f5a5a783-d999-4722-9f9f-c076846ffb6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(303, 13)\n","(303,)\n"]}],"source":["# Save the features in a matrix X and the output in an array y\n","# YOUR CODE HERE\n","X = data_df.loc[:, data_df.columns != 'output'].to_numpy()\n","print(X.shape)\n","y = data_df['output'].to_numpy()\n","print(y.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"sTP0j8yyA4yj"},"source":["## 1.4 Separation in training and test set\n","\n","As it was explained in the previous laboratory assignment, splitting the dataset is essential for an unbiased evaluation of prediction performance. In most cases, it is enough to split the dataset randomly into two subsets:\n","*   The **training set** is applied to train, or **fit**, the model. For example, it can be used to find the optimal weights, or coefficients, for linear regression, logistic regression, etc.\n","*   The **test set** is needed for an unbiased evaluation of the final model. It should not be used for fitting or validation tasks.\n","\n","The sklearn function [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) can do this work for you!\n","\n","For simplicity, use the names `X_train, X_test, y_train, y_test` for the corresponding NumPy arrays. Use $70$\\% samples for the training set and the remaining for the test set.\n","\n","**Note:** When comparing machine learning algorithms, it is desirable that they are fit and evaluated on the same subsets of the dataset. This can be achieved by fixing the seed for the pseudo-random number generator used when splitting the dataset. In the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function, this can be reached by setting the `random_state` to a specific integer value. In order to work with the same dataset as your colleagues, please use `random_state = 42` as the seed to split your data."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"SJ0cTNIoQ6nh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817462232,"user_tz":-60,"elapsed":554,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"cc844d47-522c-44b6-ee50-cbfc9049700e"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train sample: (212, 13)\n","X_test: (91, 13)\n","y_train sample: (212,)\n","y_test: (91,)\n"]}],"source":["# YOUR CODE HERE\n","import sklearn.model_selection\n","\n","X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n","# 303 rows, 13 columns\n","print(\"X_train sample:\", X_train.shape) # 70/100 = 212 samples\n","print(\"X_test:\", X_test.shape) # 30/100 = 91 samples\n","# 303 rows, 1 column\n","print(\"y_train sample:\", y_train.shape) # 70/100 = 212 samples (1 column: labels (the output))\n","print(\"y_test:\", y_test.shape) # 30/100 = 91 samples\n"]},{"cell_type":"markdown","metadata":{"id":"J0gLMh6TTN1h"},"source":["## 1.5. Scaling the inputs\n","\n","As mentioned in the previous laboratory assignment, the features or attributes of the dataset can have a wide range of different values, that is, for the dataset at hand, $\\text{F}_1 \\in [0,3]$ whereas, $\\text{F}_2 \\in [71,202]$. This can lead to some problems with specific classifiers which might lead to sub-optimal solutions. For this reason, a common pre-processing technique used in machine learning is scaling the input variables so that all end up having values in comparable ranges. This can significantly improve the performance of the classification algorithms!\n","\n","The most used scaling technique within the machine learning community is the **standardization**. This is a linear transformation that leaves each variable with $0$ mean and unit variance. In short, the idea is to transform every variable (or feature) in every sample by subtracting its mean and dividing by its standard deviation ($\\sigma$), i.e. if $\\mathbf{X}$ is one-dimensional, then:\n","\n","$$\\mathbf{X}_{\\text{s}} = \\frac{\\mathbf{X} - \\text{E}[\\mathbf{X}]}{\\sigma}$$\n","\n","\n","Please note that we seek to apply standardization to the training dataset to get it to have a similar range of variation among its different features aiming towards ​​improving the conditioning of the data matrix, which results in fewer numerical problems of the classification algorithm or of convergence of the iterative algorithms. Once we have standardized the training data, we need to transform the test data in the same way, i.e., as a function of the training dataset statistics, since otherwise, their respective samples will not be comparable.\n"]},{"cell_type":"markdown","metadata":{"id":"QafK7PDBTdAG"},"source":["\n","The module  [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), included in the library `scikit-learn`, can perform this job for you! \n"]},{"cell_type":"markdown","metadata":{"id":"7iYnpskzToQ7"},"source":["**Your task here is to standardize the data. For this purpose, in the next cell, perform the following operations.**\n","- Create an instance of `StandardScaler`\n","- Fit the scaler with the <ins>training</ins> data\n","- Create two new arrays:\n","  - `X_train_s`: It contains the result of transforming `X_train` with the scaler\n","  - `X_test_s`: It contains the result of transforming `X_test` with the (same) scaler"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"tXmsz7fMTnIi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817462233,"user_tz":-60,"elapsed":6,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"bea8a478-c7bd-4655-b090-12e31e44c392"},"outputs":[{"output_type":"stream","name":"stdout","text":["(212, 13)\n","(91, 13)\n"]}],"source":["# YOUR CODE HERE\n","from sklearn.preprocessing import StandardScaler\n","\n","sscaler = StandardScaler()\n","sscaler.fit(X_train) # we only fit the training values\n","\n","# transform to normalize all values of X (train and test)\n","X_train_s = sscaler.transform(X_train)\n","X_test_s = sscaler.transform(X_test)\n","\n","# WE NEVER NORMALIZE THE TARGET bc there is only 1 column\n","\n","# Standardized sets (same dimensions, with values between [-1,1])\n","print(X_train_s.shape)\n","print(X_test_s.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"qSiY8x0DmHsK"},"source":["# 2. Classifiers"]},{"cell_type":"markdown","metadata":{"id":"5tRhUDEtU3fE"},"source":["Once we have our data ready, let us analyze the performance obtained with different classifiers on predicting the chances of suffering a heart attack. In particular, we will experiment with the following two **parametric classifiers**:\n","\n","*   Logistic Regression\n","*   Linear Discriminant Analysis\n","\n","and this **non-parametric classifier**:\n","\n","*   $k$-Nearest Neighbors ($k$-NN)\n"]},{"cell_type":"markdown","metadata":{"id":"_TJBxFobJW2S"},"source":["## 2.1 Hyperparameter selection via cross-validation\n","\n","\n","The performance of most of the machine learning algorithms depends strongly on the selected values for the hyperparameters. \n","\n","If you remember from the previous laboratory assignment, the original dataset was divided into three subsets: **train, test, and validation**, while in this assignment, the dataset has been divided into two subsets: **train and test**. The basically idea behind this is that in the former assignment, we carried out an approximation of the common way of finding values for these hyperparameters: **cross-validation**.\n","\n","Cross validation is a commonly used procedure in machine learning to simulate the effect of training a model with a set of data and evaluate its generalization capabilities as the performance in a **separate dataset**. \n","\n","  The cross validation process involves the following steps:\n","\n","  - Randomly partition the training dataset in $N$ disjoint subsets of similar sizes. Each of this subsets is called **fold** in machine learning terminology. Hence, the term **$N$-fold cross validation**.\n","\n","  - Let us suppose we have chosen $N=3$ folds. This means the training data $(X_{\\text{train}}, Y_{\\text{train}})$ has been split in three subsets: $(X_1, Y_1)$, $(X_2, Y_2)$ and $(X_3, Y_3)$, so that $(X_1, Y_1) \\cup (X_2, Y_2) \\cup (X_3, Y_3) = (X_{\\text{train}}, Y_{\\text{train}})$.\n","\n","  - Create an instance of the model with the corresponding hyperparameters. The cross validation follows with the execution of the following loop:\n","\n","      For $n=1,2,\\dots,N$ iterations:  \n","      1. Choose $(X_n,Y_n)$ as **validation set** for iteration $n$\n","      2. Prepare a **training set** for iteration $n$ joining the rest of the subsets (excluding the validation set)\n","      3. Fit the model instance with the training set of step 2\n","      4. Evaluate  the model instance (method `score`) with the validation set of step 1\n","      5. Keep the *score* achieved in the $n$ iteration\n","\n","  - Once the loop is finished, we have $N$ scores, each corresponding to the evaluation of the model fitted in each iteration with the corresponding validation set.\n","  - Estimate the **real score** that an instance of the model fitted using all the data would yield in a separate dataset computing the **mean** and **standard deviation** of the $N$ validation scores.\n","\n","  Typical values for the number of folds include $N\\in \\{3, 5, 10\\}$. For the results obtained in this assignment, use the default $5$-fold cross validation.\n","\n","The question here is: How is cross-validation carried out in `scikit-learn`? The answer to this question is by using the class [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). This class carries out an exhaustive search over specified parameter values for an estimator (or classifier). Aiming at understing how it works, it is necessary to briefly explain some concetps:\n","\n","*   **Grids of hyperparameters**:\n","\n","      This method consists in to form a **grid** with a number of dimensions equal to the number of hyperparameters that need to be optimized. The size of each dimension of the grid is equal to the number of values in the range of the corresponding hyperparameter. Notice that this method explores **discrete** ranges for each hyperparameter.\n","\n","      In models that depend on a larger number of hyperparameters, it is necessary to be careful with the granularity of the ranges as the combinatorial explosion of the size of the grid can be hard to manage.\n","\n","\n","  \n"," \n","*   **Cross-validation to explore the grid**:\n","\n","  The grid is explored by a loop that visits all its nodes that runs a cross validation. The process details are the following:\n","\n","  For each node of the grid:  \n","  1. Create an instance of the model with the hyperparameters set to the values that define that node of the grid\n","  2. Run a **cross validation** to estimate the test performance that the model would yield if it were fitted using the values for hyperparameter that correspond to that node. \n","  3. Store the cross validation score for that node of the grid.\n","\n","  Once all the nodes of the grid have been cross validated, the procedure outputs the combination of hyperparameters that achieved the best performance in cross validation. \n","\n"," \n","\n","Before using `GridSearchCV`, let's have a look at the most important parameters it includes:\n","\n","*   `estimator`: The model on which we want to use `GridSearchCV`. For instance, a classifier such as `sklearn.neighbors.KNeighborsClassifier`.\n","*   ``param_grid``: The parameter space is given as a dictionary with parameter names (`str`) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.\n","\n","\n","* ``scoring``: Evaluation metric used to assess the performance of the cross-validated model on the test set.\n"," By default, the score function of the estimator is utilized, i.e., the `sklearn.metrics.accuracy_score` for classification and `sklearn.metrics.r2_score` for regression tasks.\n","*  ``cv``: Number of splits needed for cross-validation. By default, this parameter is set to $5$."]},{"cell_type":"markdown","metadata":{"id":"yscbkLt6Iml6"},"source":["## 2.2 Logistic regression"]},{"cell_type":"markdown","metadata":{"id":"T0Sdpag1r1NH"},"source":["### 2.2.1 Baseline\n","\n","Aiming at implementing a logistic regressor classifier in Python, we will use the class [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  included in `scikit-Learn`, which consists of a **regularized logistic regression**. \n","\n","This means that, to the loss function of the logistic regressor explained in the master class, a penalty term is added based on either L1, L2 or Elastic-Net regularization. Please note that:\n","\n","*   **L1 regularization:** It adds an L1 penalty equal to the absolute value of the magnitude of coefficients. For example, Lasso regression model implements this method. \n","*   **L2 regularization:** It adds  an L2 penalty which is equal to the square of the magnitude of coefficients. For example, Ridge or Kernel Ridge regression models implement this method.\n","*   **Elastic Net:** Both L1 and L2 penalty terms are added. This kind of regularization is not explored in this assignment.\n","\n","We can also choose to not apply any regularization by setting the parameter `penalty` to `none`.\n","\n","The regularization strength  is controlled by the parameter  `C`. It is defined as the inverse of regularization strength and it must be a positive float. Smaller values specify stronger regularization."]},{"cell_type":"markdown","metadata":{"id":"Y7yfZyfEsHHN"},"source":["**Your task here is:**\n","\n","*   Train with standardized features a logistic regressor model (with default parameters).\n","*   Calculate the accuracy rate using the test set and print it out!  Please, round off the numbers to $2$ decimal places."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VlC3vkGtr4hY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817463158,"user_tz":-60,"elapsed":928,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"7d4c4260-52be-458a-cc9e-0348d846eb0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["LR Train accuracy (without fine-tuning hypeparameters): 87.26%\n","LR Test accuracy (without fine-tuning hypeparameters): 81.32%\n"]}],"source":["# IGNORE: maybe use solver='newton-cg' as parameter in LogisticRegression\n","#YOUR CODE HERE\n","from sklearn.linear_model import LogisticRegression\n","\n","logistic_r = LogisticRegression().fit(X_train_s, y_train)\n","accuracy_LR_train = logistic_r.score(X_train_s, y_train)\n","accuracy_LR_test = logistic_r.score(X_test_s, y_test)\n","\n","print(\"LR Train accuracy (without fine-tuning hypeparameters): %.2f%%\" % (accuracy_LR_train*100))\n","print(\"LR Test accuracy (without fine-tuning hypeparameters): %.2f%%\" % (accuracy_LR_test*100))\n"]},{"cell_type":"markdown","metadata":{"id":"uAboXTrQsTld"},"source":["### 2.2.2 Tuning the hyperparameters\n","\n","In this section, it will be carried out hyperparameters optimization. The hyperparameters that will be explored before fitting the logistic regressor in this assignment will be: the parameter `penalty` and the value of `C`. Do a grid search to find out which are the best values for the aforementioned hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"__0LmlKBhJxr"},"source":["**Your task here is:**\n","\n","*   Find the optimal value of `C` and `penalty`. For this purpose, the values that will be explored aiming at optimizing the performance of the classifier are shown below:\n"," - `C` $\\in$ $[0.001,0.01,0.1,0.2,0.3,0.5,1,10,100]$\n"," - `penalty` $\\in$ {`L1`, `L2`}. \n","*   Once they have been obtained, use the attribute `best_estimator_` of `GridSearchCV` aiming at calculating the accuracy rate using the test set and print it out!  Please, round off the numbers to $2$ decimal places.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"8SrOjxDbhI8E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817463431,"user_tz":-60,"elapsed":277,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"d075c43b-d817-47e3-fe1e-31b1e8a80378"},"outputs":[{"output_type":"stream","name":"stdout","text":["Results from GridSearchCV\n","The best parameters across all searched parameters are: {'C': 0.1, 'penalty': 'l2'}\n"]}],"source":["# YOUR CODE HERE\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","parameters = {'penalty':('l1','l2'), 'C':[0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 1, 10, 100]}\n","grid_LR = GridSearchCV(estimator = LogisticRegression(), param_grid = parameters)\n","grid_LR.fit(X_train_s, y_train);\n","\n","print(\"Results from GridSearchCV\")\n","print(\"The best parameters across all searched parameters are:\", grid_LR.best_params_) # _scde(X_test_s, y_test)\n","#grid_LR.best_estimator_\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CTMukSnSlft_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817463431,"user_tz":-60,"elapsed":10,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"0084e65c-432f-4f2d-ed99-5453b83c7150"},"outputs":[{"output_type":"stream","name":"stdout","text":["82.42\n"]}],"source":["# YOUR CODE HERE\n","\n","best_score_LR = grid_LR.score(X_test_s, y_test)\n","print(round(best_score_LR*100, 2))\n"]},{"cell_type":"markdown","metadata":{"id":"vIcqPBPOGIpQ"},"source":["Do you think that the fact of fine-tuning the hyperparameters improves the classification performance? Justify your answer.\n","\n","<font color = 'green'> YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"U1L_vCpWHxP3"},"source":["## 2.3 Linear Discriminant Analysis (LDA)\n","\n","Linear Discriminant Analysis (LDA) is a generalization of Fisher's linear discriminant. Put very simply, this method projects the data points onto new axes such that these new components maximize the separability among categories while keeping the variation within each of the categories at a minimum value. The resulting combination may be used as a linear classifier, or more commonly, for dimensionality reduction before later classification.\n","\n","\n","The `scikit-Learn` [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) can do this work for you! In this case, it is not necessary hyperparameters tuning. We will train a LDA classifier with its default values."]},{"cell_type":"markdown","metadata":{"id":"-r3B-g85udZN"},"source":["**Your task here is:**\n","\n","*   Train with standardized features a LDA model (with default parameters).\n","*   Calculate the accuracy rate using the test set and print it out (use the method `score` for this purpose)! Please, round off the numbers to $2$ decimal places.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GVWSW_OwvEZ1","executionInfo":{"status":"ok","timestamp":1669817463432,"user_tz":-60,"elapsed":7,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}}},"outputs":[],"source":["#Training the model\n","#YOUR CODE HERE\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","# we create the model always with th training values\n","lda = LinearDiscriminantAnalysis().fit(X_train_s, y_train)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Hg4Kt8q6vfaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817463683,"user_tz":-60,"elapsed":257,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"65c44b72-70a1-4f47-9897-2da2dafad132"},"outputs":[{"output_type":"stream","name":"stdout","text":["80.22\n"]}],"source":["#Testing the model\n","#YOUR CODE HERE\n","\n","best_lda = lda.score(X_test_s, y_test) # we score with the test always\n","print(round(best_lda*100, 2))\n"]},{"cell_type":"markdown","metadata":{"id":"EWN-UykPwAEt"},"source":["Having a look at the train accuracy, it is easy to observe that the problem is not perfectly linearly separable, but let us check it graphically (for the training set). Project each standardized training sample onto one axis (`n_components=1` in LDA model) and represent it (indicating its category or class).\n","\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qqjTvXjql5iK","colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"status":"ok","timestamp":1669817464362,"user_tz":-60,"elapsed":690,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"67721bc2-5c59-4c45-ae31-7c8bf97e6c23"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"image/png":{"width":378,"height":261},"needs_background":"light"}}],"source":["#YOUR CODE HERE\n","lda_D1 = LinearDiscriminantAnalysis(n_components=1)\n","X_lda = lda_D1.fit_transform(X_train_s, y_train)\n","\n","plt.xlabel('LDA1')\n","X_lda_clase_0 = X_lda[y_train==0,0]\n","X_lda_clase_1 = X_lda[y_train==1,0]\n","\n","y0 = np.ones((X_lda_clase_0.shape[0], 1))\n","y1 = np.ones((X_lda_clase_1.shape[0], 1))\n","\n","plt.plot(X_lda_clase_0, y0, '*b')\n","plt.plot(X_lda_clase_1, y1, 'om')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rU8DYvUe2vzJ"},"source":["Why is not linearly separable?\n","\n","<font color = 'green'> YOUR ANSWER HERE\n","\n","<font color = 'black'> It is not super precise, there is some Pfa"]},{"cell_type":"markdown","metadata":{"id":"u9qZZJ9JJZfl"},"source":["## 2.4 $k$-NN classifier\n","\n","Finally, we will explore the $k$-Nearest Neighbours ($k$-NN) algorithm. This is, by far, one of the easiest non-parametric machine learning models to understand. \n","\n","The discriminant function for $k$-NN is constructed in an algorithmic way. For every observation $\\mathbf{x}_{\\text{test}}$ that needs to be classified with $k$-NN, the following steps are carried out (once the appropriate number of neighbours $-$$k$$-$ has been selected):\n","\n","1. Compute the (Euclidean) distance between $\\mathbf{x}_{\\text{test}}$ and all the observations in the\n","training set.\n","2. Sort these distances in ascending order.\n","3. Find the training observations that occupy the first $k$ positions in the\n","ranking of distances to $\\mathbf{x}_{\\text{test}}$. Let’s denote by $\\mathcal{I}_k = \\{i_1, i_2,\\ldots, i_k\\}$ the indices\n","of these training observations in the training set.\n","4. Retrieve the true targets of the $k$-NN: ${y_{i_1},y_{i_2},\\ldots,y_{i_k}}$.\n","5. $\\widehat{y}_{\\text{test}}$ = mode\n","$\\{y_{i_1},y_{i_2},\\ldots,y_{i_k}\\}$.\n","\n","The last step can be modified replacing the mode by a voting, where each\n","neighbor $\\mathbf{x}_{i_{j}}$ votes for its class with its vote inversely proportional to the\n","distance between $\\mathbf{x}_{i_{j}}$ \n","and $\\mathbf{x}_{\\text{test}}$.\n","\n","\n","\n","So easy, isn't it? We are ready to implement our $k$-NN algorithm!\n","\n","Here we will implement a $k$-NN algorithm by fine-tuning its two main hyperparameters:\n","\n","*   `n_neighbors`: Number of neighbors to be used in the experiments (that is, the value of $k$).\n","*   `weights`: Weight function used in prediction. In this assignment, it can take two possible values:\n","  *   `uniform`: uniform weights. All samples in each neighborhood are weighted equally.\n","  *   `distance`: weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n","\n","The `scikit-Learn` [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) can do this work for you!"]},{"cell_type":"markdown","metadata":{"id":"_oihLGH0J7b8"},"source":["**Your task here is:**\n","\n","*   Find the optimal value of `n_neighbors` (that is, the value of $k$) and `weights`. For this purpose, the values that will be explored aiming at optimizing the performance of the classifier are shown below:\n"," - `n_neighbors` $\\in$ $[1, 2, 3,\\ldots,9, 10]$\n"," - `weights` $\\in$ {`uniform`, `distance`}\n","*   Once they have been obtained, use the attribute `best_estimator_` of `GridSearchCV` aiming at calculating the accuracy rate using the test set and print it out!  Please, round off the numbers to $2$ decimal places."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ag5LRIviGB_W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817465344,"user_tz":-60,"elapsed":986,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"dab16b8c-7d70-4d37-c932-f4c963ecec2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["K np vector is: [ 1  2  3  4  5  6  7  8  9 10]\n","Results from GridSearchCV\n","The best parameters across all searched parameters are: {'n_neighbors': 3, 'weights': 'uniform'}\n"]}],"source":["#Fine-tuning hyperparameters\n","#YOUR CODE HERE\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","K = np.arange(1, 11, 1) # np.arange([start, end), step) ---> prints: [1  2  3  4  5  6  7  8  9 10]\n","print('K np vector is:',K)\n","parameters = {'weights':('uniform', 'distance'), 'n_neighbors': K}\n","grid_KNN = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = parameters, cv = 4).fit(X_train_s, y_train)\n","\n","print(\"Results from GridSearchCV\")\n","print(\"The best parameters across all searched parameters are:\", grid_KNN.best_params_)\n"]},{"cell_type":"markdown","metadata":{"id":"uNbREFigm9Pt"},"source":["Which are the best hyperparameters that you have obtained?\n","\n","<font color = 'green'> YOUR ANSWER HERE"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"lJkX1Q1BGpNe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669817465345,"user_tz":-60,"elapsed":10,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}},"outputId":"a601d087-3285-41c8-9000-82ab02676335"},"outputs":[{"output_type":"stream","name":"stdout","text":["83.52 %\n"]}],"source":["#Testing the model\n","#YOUR CODE HERE\n","\n","score_grid_KNN = grid_KNN.score(X_test_s, y_test)\n","\n","print(round(score_grid_KNN*100, 2),'%')\n"]},{"cell_type":"markdown","metadata":{"id":"Ql-jDnXHHYRB"},"source":["According to the results that you have obtained, what is the best classifier for this classification task? Justify your answer.\n","\n","<font color = 'green'> YOUR ANSWER HERE\n","<font color = 'black'> "]},{"cell_type":"markdown","metadata":{"id":"RK-CYbewmMwm"},"source":["# 3. Other classification performance metrics: ROC and AUC"]},{"cell_type":"markdown","metadata":{"id":"ITpCx5GCsnes"},"source":["In this last section, two different metrics commonly used for binary classification problems will be explored.\n","\n","The **Receiver Operating Characteristic** (ROC) curve is a graphical plot used for binary classification that shows the relation between the probability of detection ($P_{\\text{D}}$) versus\n","the probability of false alarm ($P_{\\text{FA}}$) , for different decision thresholds. This means that each point in the curve corresponds to a work point.\n","\n","The ROC curve is then determined by calculating the probability of detection of each data observation and using different values of thresholds to obtain the labels.\n","Furthermore, to obtain a performance metric similar to accuracy, it is possible to compute the Area Under the Curve (AUC) metric, which determines what is the total area under the ROC curve.\n","\n","Once again, there is a function available in `scikit-learn` to calculate the [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) and another to calculate the [`auc`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc). \n","\n","$\\underline{\\text{Hint}}$: Please note that the parameter `y_score` required in `roc_curve` represents the probability estimates of the positive class. In binary classification case, the method `predict_proba` returns probability estimates. In particular, it predicts the probabibility for a sample to be negative and positive, and the second column shows how much probability of this sample belongs to positive class. When passing only positive probability, ROC evaluates on different thresholds and checks if given probability $>$ threshold (say $0.5$), it belongs to positive class otherwise it belongs to negative class. \n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"FREOL5FMclhT","executionInfo":{"status":"ok","timestamp":1669817583660,"user_tz":-60,"elapsed":219,"user":{"displayName":"RODRIGO DE LAMA FERNANDEZ","userId":"09889274295334891540"}}},"outputs":[],"source":["# Plot the ROC curve and calculate the AUC for the LR classifier.\n","#YOUR CODE HERE\n","#from sklearn.metrics import roc_curve, auc\n","from sklearn import metrics\n","# fpr_LR, tpr_LR, thresholds = roc_curve(y_test, grid_LR.best_estimator_.predict_proba(X_test_s)[:,1])\n","# roc_auc_LR = auc(fpr_LR, tpr_LR)\n","# plt.plot(fpr_LR, tpr_LR, color=\"purple\", lw=2, label=\"ROC curve LR (area = %0.3f)\" % roc_auc_LR)\n","\n","# fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, grid_LR.best_estimator_.predict_proba(X_test_s)[:,1]) #probabiblidad de que la muestra pertenezca a 1 o 0\n","# roc_auc_LR = auc(fpr_knn, tpr_knn)\n","# print(roc_auc_LR)\n","# plt.plot(tpr_knn,fpr_knn, label='KNN')\n","# plt.ylabel('False Positive Rate')\n","# plt.xlabel('True Positive Rate')\n","\n","\n","# Plot the ROC curve and calculate the AUC for the LR classifier.\n","#YOUR CODE HERE\n","from sklearn import metrics\n","fpr_LR, tpr_LR, thresholds = metrics.roc_curve(y_test, grid_LR.best_estimator_.predict_proba(X_test_s)[:,1])\n","roc_auc_LR = metrics.auc(fpr_LR, tpr_LR)\n","# plt.plot(fpr_LR, tpr_LR, color=\"purple\", lw=2, label=\"ROC curve LR (area = %0.3f)\" % roc_auc_LR)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"0e5b5b920195072d4a4eec1d5ff9e5f87252d2725e2a57da6939cd4fcd91d4cd"}}},"nbformat":4,"nbformat_minor":0}